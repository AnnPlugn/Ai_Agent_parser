import json
from typing import TypedDict, List, Dict, Optional

from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import os
import logging
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import load_env_file, LLMProvider, LLMConfig
from prompts import PROMPTS

# –ò–º–ø–æ—Ä—Ç –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –ø–∞—Ä—Å–µ—Ä–æ–≤
from parsers import (
    extract_dependencies,
    extract_prompts,
    load_requirements,
    get_project_structure,
    extract_text_from_pdf,
    extract_text_from_docx,
    extract_text_from_txt,
    extract_csv_requirements,
    extract_excel_requirements,
    smart_parse_txt_bpmn,
    parse_md,
    extract_java_dependencies,
    extract_js_dependencies,
    extract_cpp_dependencies,
    extract_business_requirements,
    extract_project_description,
    should_process_file,
    get_supported_extensions
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ .env —Ñ–∞–π–ª–∞
load_env_file()


def create_llm_client(config: LLMConfig):
    """–°–æ–∑–¥–∞–Ω–∏–µ LLM –∫–ª–∏–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    try:
        if config.provider == LLMProvider.GIGACHAT:
            from langchain_gigachat import GigaChat
            if not (config.cert_file and config.key_file):
                raise ValueError("–î–ª—è GigaChat –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã cert_file –∏ key_file")
            return GigaChat(
                base_url=config.base_url,
                cert_file=config.cert_file,
                key_file=config.key_file,
                model=config.model,
                temperature=config.temperature,
                top_p=config.top_p,
                verify_ssl_certs=config.verify_ssl_certs,
                profanity_check=config.profanity_check,
                streaming=config.streaming
            )
        else:
            from langchain_openai import OpenAI
            return OpenAI(
                base_url=config.base_url,
                model=config.model,
                temperature=config.temperature,
                max_tokens=config.max_tokens,
                timeout=config.timeout,
                api_key=config.api_key
            )
    except ImportError as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: {e}")
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞: {e}")
        raise


# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≥—Ä–∞—Ñ–∞
class ParserState(TypedDict):
    directory_path: str
    files_list: List[str]
    empty_files: List[str]
    dependencies: Dict[str, Dict[str, List[str]]]
    prompts: Dict[str, List[str]]
    requirements: Dict[str, str]
    business_requirements: Dict[str, str]
    project_descriptions: Dict[str, str]
    analysis: str
    final_report: str
    project_structure: Dict[str, List[str]]
    file_stats: Dict[str, int]


def chunk_text(text: str, chunk_size: int = 80000, overlap: float = 0.2) -> List[str]:
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
    chunks = []
    overlap_size = int(chunk_size * overlap)
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start += chunk_size - overlap_size
    return chunks


def get_files_node(state: ParserState) -> ParserState:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    files = []
    empty_files = []
    supported_extensions = get_supported_extensions()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    all_supported = []
    for category in supported_extensions.values():
        all_supported.extend(category)

    project_structure = get_project_structure(state['directory_path'])
    file_stats = {}

    for root, dirs, filenames in os.walk(state['directory_path']):
        if '.git' in dirs:
            dirs.remove('.git')
        if '__pycache__' in dirs:
            dirs.remove('__pycache__')
        if 'node_modules' in dirs:
            dirs.remove('node_modules')

        for filename in filenames:
            file_path = os.path.join(root, filename)
            ext = os.path.splitext(filename)[1].lower()

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã
            if not should_process_file(file_path):
                continue

            try:
                file_size = os.path.getsize(file_path)
                if file_size == 0:
                    empty_files.append(file_path)
                    continue
                elif file_size > 10 * 1024 * 1024:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –±–æ–ª—å—à–µ 10MB
                    logger.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª: {file_path} ({file_size} –±–∞–π—Ç)")
                    continue

                files.append(file_path)

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
                if ext not in file_stats:
                    file_stats[ext] = 0
                file_stats[ext] += 1

            except OSError as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª—É {file_path}: {e}")
                continue

    state['files_list'] = files
    state['empty_files'] = empty_files
    state['project_structure'] = project_structure
    state['file_stats'] = file_stats

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(files)} –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ {len(empty_files)} –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤")
    logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∞–π–ª–æ–≤: {file_stats}")
    return state


def analyze_files_node(state: ParserState) -> ParserState:
    """–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤"""
    dependencies = {}
    prompts = {}
    requirements = {}
    business_requirements = {}
    project_descriptions = {}

    # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ —Ç–∏–ø–∞–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    code_files = []
    document_files = []
    data_files = []

    supported = get_supported_extensions()

    for file in state['files_list']:
        ext = os.path.splitext(file)[1].lower()
        if ext in supported['code_files']:
            code_files.append(file)
        elif ext in supported['document_files']:
            document_files.append(file)
        elif ext in supported['data_files']:
            data_files.append(file)

    logger.info(
        f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤: –∫–æ–¥={len(code_files)}, –¥–æ–∫—É–º–µ–Ω—Ç—ã={len(document_files)}, –¥–∞–Ω–Ω—ã–µ={len(data_files)}")

    with ThreadPoolExecutor(max_workers=4) as executor:
        # –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è –∫–æ–¥–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        future_to_file = {}

        for file in code_files:
            future_to_file[executor.submit(safe_extract_dependencies, file)] = file

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        for file in state['files_list']:
            future_to_file[executor.submit(safe_extract_prompts, file)] = file

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏ –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        for file in document_files + data_files:
            future_to_file[executor.submit(safe_load_requirements, file)] = file
            future_to_file[executor.submit(safe_extract_business_requirements, file)] = file
            future_to_file[executor.submit(safe_extract_project_description, file)] = file

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        for future in as_completed(future_to_file):
            file = future_to_file[future]
            try:
                result = future.result()
                task_type = determine_task_type(future, file)

                if task_type == 'dependencies':
                    dependencies[file] = result
                elif task_type == 'prompts':
                    if result:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–º–ø—Ç—ã
                        prompts[file] = result
                elif task_type == 'requirements':
                    if result and len(result.strip()) > 10:
                        requirements[file] = result
                elif task_type == 'business_requirements':
                    if result and len(result.strip()) > 10:
                        business_requirements[file] = result
                elif task_type == 'project_description':
                    if result and len(result.strip()) > 10:
                        project_descriptions[file] = result

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file}: {e}")

    state['dependencies'] = dependencies
    state['prompts'] = prompts
    state['requirements'] = requirements
    state['business_requirements'] = business_requirements
    state['project_descriptions'] = project_descriptions

    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
    logger.info(f"  - –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {len(dependencies)} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"  - –ü—Ä–æ–º–ø—Ç—ã: {len(prompts)} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"  - –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: {len(requirements)} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"  - –ë–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: {len(business_requirements)} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"  - –û–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞: {len(project_descriptions)} —Ñ–∞–π–ª–æ–≤")

    return state


def safe_extract_dependencies(file_path: str) -> Dict[str, List[str]]:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        return extract_dependencies(file_path)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏–∑ {file_path}: {e}")
        return {"libraries": [], "functions": []}


def safe_extract_prompts(file_path: str) -> List[str]:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        return extract_prompts(file_path)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ {file_path}: {e}")
        return []


def safe_load_requirements(file_path: str) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        return load_requirements(file_path)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏–∑ {file_path}: {e}")
        return ""


def safe_extract_business_requirements(file_path: str) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        return extract_business_requirements(file_path)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏–∑ {file_path}: {e}")
        return ""


def safe_extract_project_description(file_path: str) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        return extract_project_description(file_path)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –∏–∑ {file_path}: {e}")
        return ""


def determine_task_type(future, file_path: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–¥–∞—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ —Ñ–∞–π–ª–∞"""
    # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –∫–∞–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –±—ã–ª–∞ –≤—ã–∑–≤–∞–Ω–∞
    # –ó–¥–µ—Å—å –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é —Ñ–∞–π–ª–∞ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
    ext = os.path.splitext(file_path)[1].lower()
    filename = os.path.basename(file_path).lower()

    if 'dependencies' in str(future):
        return 'dependencies'
    elif 'prompts' in str(future):
        return 'prompts'
    elif 'business' in filename or 'requirements' in filename:
        return 'business_requirements'
    elif 'readme' in filename or 'description' in filename:
        return 'project_description'
    else:
        return 'requirements'


def llm_analysis_node(state: ParserState, llm) -> ParserState:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π LLM –∞–Ω–∞–ª–∏–∑ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω–æ–≤—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM
        dep_content = prepare_dependencies_content(state['dependencies'])
        prompt_content = prepare_prompts_content(state['prompts'])
        req_content = prepare_requirements_content(state['requirements'])
        business_content = prepare_business_content(state['business_requirements'])
        description_content = prepare_description_content(state['project_descriptions'])
        structure_content = prepare_structure_content(state['project_structure'])
        stats_content = prepare_stats_content(state['file_stats'])

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        all_content = f"""
=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–ï–ö–¢–ê ===
{stats_content}

=== –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ü–†–û–ï–ö–¢–ê ===
{structure_content}

=== –û–ü–ò–°–ê–ù–ò–Ø –ü–†–û–ï–ö–¢–ê ===
{description_content}

=== –ë–ò–ó–ù–ï–°-–¢–†–ï–ë–û–í–ê–ù–ò–Ø ===
{business_content}

=== –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø ===
{req_content}

=== –ó–ê–í–ò–°–ò–ú–û–°–¢–ò –ò –ö–û–î ===
{dep_content}

=== –ü–†–û–ú–ü–¢–´ –ò –®–ê–ë–õ–û–ù–´ ===
{prompt_content}
"""

        # –ß–∞–Ω–∫–∏–Ω–≥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤
        max_input_size = 100000
        chunks = chunk_text(all_content, chunk_size=80000, overlap=0.2)

        system_prompt_template = ChatPromptTemplate.from_template(PROMPTS['system_prompt'])
        chain = system_prompt_template | llm | StrOutputParser()

        analysis_chunks = []
        for i, chunk in enumerate(chunks):
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫ {i + 1}/{len(chunks)}")
            try:
                analysis_chunk = chain.invoke({"content": chunk})
                analysis_chunks.append(analysis_chunk)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞ {i + 1}: {e}")
                analysis_chunks.append(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞ {i + 1}: {str(e)}")

        # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤
        if len(analysis_chunks) > 1:
            summary_prompt = ChatPromptTemplate.from_template(
                "–û–±—ä–µ–¥–∏–Ω–∏ –∏ —Å—É–º–º–∏—Ä—É–π —ç—Ç–∏ –∞–Ω–∞–ª–∏–∑—ã –≤ –µ–¥–∏–Ω—ã–π coherent–Ω—ã–π –æ—Ç—á–µ—Ç: {analyses}"
            )
            summary_chain = summary_prompt | llm | StrOutputParser()
            analysis = summary_chain.invoke({"analyses": "\n\n".join(analysis_chunks)})
        else:
            analysis = analysis_chunks[0] if analysis_chunks else "–ê–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å –ø–æ–º–æ—â—å—é judge
        judge_result = run_judge_validation(llm, analysis, all_content)

        # –£–ª—É—á—à–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ judge
        final_analysis = enhance_analysis_with_judge_feedback(
            analysis, judge_result, state, dep_content, prompt_content,
            req_content, business_content, description_content, structure_content
        )

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        save_analysis_results(final_analysis, state)

        state['analysis'] = final_analysis
        logger.info("‚úÖ LLM –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ LLM –∞–Ω–∞–ª–∏–∑–µ: {e}")
        error_analysis = generate_error_analysis(e, state)
        state['analysis'] = error_analysis
        save_analysis_results(error_analysis, state)

    return state


def prepare_dependencies_content(dependencies: Dict[str, Dict[str, List[str]]]) -> str:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    if not dependencies:
        return "–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    content = "–ê–ù–ê–õ–ò–ó –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô –ò –ê–†–•–ò–¢–ï–ö–¢–£–†–´:\n\n"

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º
    all_libs = set()
    all_functions = set()

    for file_path, deps in dependencies.items():
        all_libs.update(deps.get('libraries', []))
        all_functions.update(deps.get('functions', []))

    content += f"–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n"
    content += f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫: {len(all_libs)}\n"
    content += f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π: {len(all_functions)}\n\n"

    if all_libs:
        content += f"–ö–ª—é—á–µ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: {', '.join(sorted(list(all_libs))[:20])}\n\n"

    # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Ñ–∞–π–ª–∞–º (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è)
    content += "–î–µ—Ç–∞–ª–∏ –ø–æ —Ñ–∞–π–ª–∞–º:\n"
    for file_path, deps in list(dependencies.items())[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        rel_path = os.path.relpath(file_path)
        content += f"\n{rel_path}:\n"
        if deps['libraries']:
            content += f"  –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏: {', '.join(deps['libraries'][:10])}\n"
        if deps['functions']:
            content += f"  –§—É–Ω–∫—Ü–∏–∏: {', '.join(deps['functions'][:10])}\n"

    return content


def prepare_prompts_content(prompts: Dict[str, List[str]]) -> str:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø—Ä–æ–º–ø—Ç–æ–≤"""
    if not prompts:
        return "–ü—Ä–æ–º–ø—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    content = "–ò–ó–í–õ–ï–ß–ï–ù–ù–´–ï –ü–†–û–ú–ü–¢–´ –ò –®–ê–ë–õ–û–ù–´:\n\n"
    total_prompts = sum(len(prompt_list) for prompt_list in prompts.values())
    content += f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ–º–ø—Ç–æ–≤: {total_prompts}\n\n"

    for file_path, prompt_list in prompts.items():
        if not prompt_list:
            continue

        rel_path = os.path.relpath(file_path)
        content += f"\n=== {rel_path} ({len(prompt_list)} –ø—Ä–æ–º–ø—Ç–æ–≤) ===\n"

        for i, prompt in enumerate(prompt_list[:5], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 5 –ø—Ä–æ–º–ø—Ç–∞–º–∏ –Ω–∞ —Ñ–∞–π–ª
            # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
            truncated_prompt = prompt[:500] + "..." if len(prompt) > 500 else prompt
            content += f"\n{i}. {truncated_prompt}\n"

        if len(prompt_list) > 5:
            content += f"\n... –∏ –µ—â–µ {len(prompt_list) - 5} –ø—Ä–æ–º–ø—Ç–æ–≤\n"

    return content


def prepare_requirements_content(requirements: Dict[str, str]) -> str:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"""
    if not requirements:
        return "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    content = "–¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ò –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–Ø:\n\n"

    for file_path, req in requirements.items():
        rel_path = os.path.relpath(file_path)
        content += f"\n=== {rel_path} ===\n"

        # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        if len(req) > 2000:
            content += req[:2000] + f"\n... [–æ–±—Ä–µ–∑–∞–Ω–æ, –ø–æ–ª–Ω–∞—è –¥–ª–∏–Ω–∞: {len(req)} —Å–∏–º–≤–æ–ª–æ–≤]\n"
        else:
            content += req + "\n"

    return content


def prepare_business_content(business_requirements: Dict[str, str]) -> str:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"""
    if not business_requirements:
        return "–ë–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    content = "–ë–ò–ó–ù–ï–°-–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ò –ü–†–û–¶–ï–°–°–´:\n\n"

    for file_path, req in business_requirements.items():
        rel_path = os.path.relpath(file_path)
        content += f"\n=== {rel_path} ===\n"

        if len(req) > 3000:
            content += req[:3000] + f"\n... [–æ–±—Ä–µ–∑–∞–Ω–æ, –ø–æ–ª–Ω–∞—è –¥–ª–∏–Ω–∞: {len(req)} —Å–∏–º–≤–æ–ª–æ–≤]\n"
        else:
            content += req + "\n"

    return content


def prepare_description_content(project_descriptions: Dict[str, str]) -> str:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏–π –ø—Ä–æ–µ–∫—Ç–∞"""
    if not project_descriptions:
        return "–û–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    content = "–û–ü–ò–°–ê–ù–ò–Ø –ò –û–ë–ó–û–† –ü–†–û–ï–ö–¢–ê:\n\n"

    for file_path, desc in project_descriptions.items():
        rel_path = os.path.relpath(file_path)
        content += f"\n=== {rel_path} ===\n"
        content += desc + "\n"

    return content


def prepare_structure_content(project_structure: Dict[str, List[str]]) -> str:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞"""
    content = "–°–¢–†–£–ö–¢–£–†–ê –ü–†–û–ï–ö–¢–ê:\n\n"

    for dir_path, files in project_structure.items():
        content += f"{dir_path}:\n"
        for file in files[:20]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
            content += f"  - {file}\n"
        if len(files) > 20:
            content += f"  ... –∏ –µ—â–µ {len(files) - 20} —Ñ–∞–π–ª–æ–≤\n"
        content += "\n"

    return content


def prepare_stats_content(file_stats: Dict[str, int]) -> str:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ñ–∞–π–ª–∞–º"""
    if not file_stats:
        return "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∞–π–ª–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."

    content = "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–ò–ü–ê–ú –§–ê–ô–õ–û–í:\n"
    total_files = sum(file_stats.values())
    content += f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}\n\n"

    for ext, count in sorted(file_stats.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_files) * 100
        content += f"{ext}: {count} —Ñ–∞–π–ª–æ–≤ ({percentage:.1f}%)\n"

    return content


def run_judge_validation(llm, analysis: str, content: str) -> Dict:
    """–ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ judge"""
    try:
        judge_prompt_template = ChatPromptTemplate.from_template(PROMPTS['judge'])
        judge_chain = judge_prompt_template | llm | StrOutputParser()

        judge_input = {
            "system_prompt": PROMPTS['system_prompt'],
            "analysis": analysis[:5000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è judge
            "content": content[:10000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è judge
        }

        judge_output = judge_chain.invoke(judge_input)

        try:
            judge_result = json.loads(judge_output)
        except json.JSONDecodeError:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –æ—Ç–≤–µ—Ç –æ—Ç judge")
            judge_result = {"status": "unknown", "missing": []}

        return judge_result

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ judge –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        return {"status": "error", "missing": []}


def enhance_analysis_with_judge_feedback(analysis: str, judge_result: Dict, state: ParserState,
                                         dep_content: str, prompt_content: str, req_content: str,
                                         business_content: str, description_content: str,
                                         structure_content: str) -> str:
    """–£–ª—É—á—à–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç judge"""

    if judge_result.get('status') == 'good':
        return analysis

    enhanced_analysis = analysis
    missing = judge_result.get('missing', [])

    if "prompts" in missing and state['prompts']:
        enhanced_analysis += f"\n\n=== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–û–ú–ü–¢–´ ===\n{prompt_content}"

    if "dependencies" in missing and state['dependencies']:
        enhanced_analysis += f"\n\n=== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ó–ê–í–ò–°–ò–ú–û–°–¢–ò ===\n{dep_content}"

    if "structure" in missing:
        enhanced_analysis += f"\n\n=== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê ===\n{structure_content}"

    if "requirements" in missing and state['requirements']:
        enhanced_analysis += f"\n\n=== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø ===\n{req_content}"

    if "business" in missing and state['business_requirements']:
        enhanced_analysis += f"\n\n=== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ë–ò–ó–ù–ï–°-–¢–†–ï–ë–û–í–ê–ù–ò–Ø ===\n{business_content}"

    if "description" in missing and state['project_descriptions']:
        enhanced_analysis += f"\n\n=== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –û–ü–ò–°–ê–ù–ò–Ø –ü–†–û–ï–ö–¢–ê ===\n{description_content}"

    return enhanced_analysis


def save_analysis_results(analysis: str, state: ParserState):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
    try:
        analysis_file = "llm_analysis.txt"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            f.write(analysis)
        state['analysis_file'] = analysis_file

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        details_file = "analysis_details.json"
        details = {
            "file_stats": state['file_stats'],
            "prompts_count": sum(len(prompts) for prompts in state['prompts'].values()),
            "dependencies_count": len(state['dependencies']),
            "requirements_count": len(state['requirements']),
            "business_requirements_count": len(state['business_requirements']),
            "project_descriptions_count": len(state['project_descriptions']),
            "total_files": len(state['files_list']),
            "empty_files": len(state['empty_files'])
        }

        with open(details_file, 'w', encoding='utf-8') as f:
            json.dump(details, f, ensure_ascii=False, indent=2)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞: {e}")


def generate_error_analysis(error: Exception, state: ParserState) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏"""
    error_msg = f"–û—à–∏–±–∫–∞ LLM –∞–Ω–∞–ª–∏–∑–∞: {str(error)}\n\n"

    if "404" in str(error):
        error_msg += "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
        error_msg += "- –ù–µ–≤–µ—Ä–Ω—ã–π URL API\n"
        error_msg += "- –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä LLM\n"
        error_msg += "- –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç–µ–≤—ã–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º\n\n"

    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ
    error_msg += "–ë–ê–ó–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ü–†–û–ï–ö–¢–ï:\n\n"
    error_msg += f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(state['files_list'])}\n"
    error_msg += f"–ü—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(state['empty_files'])}\n"

    if state['file_stats']:
        error_msg += f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∞–π–ª–æ–≤: {state['file_stats']}\n"

    if state['dependencies']:
        error_msg += f"–§–∞–π–ª–æ–≤ —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏: {len(state['dependencies'])}\n"

    if state['prompts']:
        total_prompts = sum(len(prompts) for prompts in state['prompts'].values())
        error_msg += f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ–º–ø—Ç–æ–≤: {total_prompts}\n"

    return error_msg


def compile_report_node(state: ParserState) -> ParserState:
    """–ö–æ–º–ø–∏–ª—è—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
    report = "# ü§ñ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –ò–ò-–∞–≥–µ–Ω—Ç–∞/–ø—Ä–æ–µ–∫—Ç–∞\n\n"

    # –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ
    report += generate_executive_summary(state)

    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
    report += "## üìÅ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞\n\n"
    report += generate_architecture_section(state)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∞–π–ª–æ–≤
    report += "## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∞–π–ª–æ–≤\n\n"
    report += generate_file_statistics(state)

    # –û–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞
    if state['project_descriptions']:
        report += "## üìã –û–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞\n\n"
        report += generate_project_descriptions_section(state)

    # –ë–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
    if state['business_requirements']:
        report += "## üíº –ë–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è\n\n"
        report += generate_business_requirements_section(state)

    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
    if state['requirements']:
        report += "## ‚öôÔ∏è –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è\n\n"
        report += generate_technical_requirements_section(state)

    # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–∏
    report += "## üîß –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n\n"
    report += generate_dependencies_section(state)

    # –ü—Ä–æ–º–ø—Ç—ã –∏ —à–∞–±–ª–æ–Ω—ã
    if state['prompts']:
        report += "## üéØ –ü—Ä–æ–º–ø—Ç—ã –∏ —à–∞–±–ª–æ–Ω—ã –ò–ò\n\n"
        report += generate_prompts_section(state)

    # LLM –∞–Ω–∞–ª–∏–∑
    report += "## üß† –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ò–ò\n\n"
    report += state['analysis'] + "\n\n"

    # –ü—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã
    if state['empty_files']:
        report += "## ‚ö†Ô∏è –ü—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã\n\n"
        report += generate_empty_files_section(state)

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    report += generate_recommendations_section(state)

    state['final_report'] = report
    return state


def generate_executive_summary(state: ParserState) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ"""
    summary = "## üìà –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ\n\n"

    total_files = len(state['files_list'])
    prompts_count = sum(len(prompts) for prompts in state['prompts'].values())
    deps_count = len(state['dependencies'])

    summary += f"**–û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞:**\n"
    summary += f"- üìÅ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: **{total_files}**\n"
    summary += f"- üéØ –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –ò–ò: **{prompts_count}**\n"
    summary += f"- üîß –§–∞–π–ª–æ–≤ —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏: **{deps_count}**\n"
    summary += f"- üìã –§–∞–π–ª–æ–≤ —Å –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏: **{len(state['business_requirements'])}**\n"
    summary += f"- ‚öôÔ∏è –§–∞–π–ª–æ–≤ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏: **{len(state['requirements'])}**\n"
    summary += f"- üìñ –§–∞–π–ª–æ–≤ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –ø—Ä–æ–µ–∫—Ç–∞: **{len(state['project_descriptions'])}**\n\n"

    return summary


def generate_architecture_section(state: ParserState) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""
    content = ""

    for dir_path, files in state['project_structure'].items():
        content += f"### üìÇ {dir_path}\n\n"

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
        files_by_ext = {}
        for file in files:
            ext = os.path.splitext(file)[1] or '–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è'
            if ext not in files_by_ext:
                files_by_ext[ext] = []
            files_by_ext[ext].append(file)

        for ext, ext_files in sorted(files_by_ext.items()):
            content += f"**{ext} —Ñ–∞–π–ª—ã ({len(ext_files)}):**\n"
            for file in ext_files[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥
                content += f"- `{file}`\n"
            if len(ext_files) > 10:
                content += f"- ... –∏ –µ—â–µ {len(ext_files) - 10} —Ñ–∞–π–ª–æ–≤\n"
            content += "\n"

    return content


def generate_file_statistics(state: ParserState) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ñ–∞–π–ª–æ–≤"""
    if not state['file_stats']:
        return "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.\n\n"

    content = "| –¢–∏–ø —Ñ–∞–π–ª–∞ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –ü—Ä–æ—Ü–µ–Ω—Ç |\n"
    content += "|-----------|------------|----------|\n"

    total_files = sum(state['file_stats'].values())

    for ext, count in sorted(state['file_stats'].items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_files) * 100
        content += f"| `{ext}` | {count} | {percentage:.1f}% |\n"

    content += f"\n**–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤:** {total_files}\n\n"
    return content


def generate_project_descriptions_section(state: ParserState) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π –ø—Ä–æ–µ–∫—Ç–∞"""
    content = ""

    for file_path, description in state['project_descriptions'].items():
        rel_path = os.path.relpath(file_path)
        content += f"### üìÑ {rel_path}\n\n"
        content += f"{description}\n\n"
        content += "---\n\n"

    return content


def generate_business_requirements_section(state: ParserState) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"""
    content = ""

    for file_path, requirements in state['business_requirements'].items():
        rel_path = os.path.relpath(file_path)
        content += f"### üíº {rel_path}\n\n"

        if len(requirements) > 5000:
            content += f"{requirements[:5000]}\n\n*[–ü–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {len(requirements)}]*\n\n"
        else:
            content += f"{requirements}\n\n"

        content += "---\n\n"

    return content


def generate_technical_requirements_section(state: ParserState) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"""
    content = ""

    for file_path, requirements in state['requirements'].items():
        rel_path = os.path.relpath(file_path)
        content += f"### ‚öôÔ∏è {rel_path}\n\n"

        if len(requirements) > 3000:
            content += f"{requirements[:3000]}\n\n*[–ü–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ 3000 —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {len(requirements)}]*\n\n"
        else:
            content += f"{requirements}\n\n"

        content += "---\n\n"

    return content


def generate_dependencies_section(state: ParserState) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    content = ""

    if not state['dependencies']:
        return "–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\n\n"

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    all_libs = set()
    all_functions = set()

    for deps in state['dependencies'].values():
        all_libs.update(deps.get('libraries', []))
        all_functions.update(deps.get('functions', []))

    content += f"**–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**\n"
    content += f"- üìö –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫: {len(all_libs)}\n"
    content += f"- ‚ö° –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π: {len(all_functions)}\n\n"

    if all_libs:
        content += f"**–ö–ª—é—á–µ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:** `{', '.join(sorted(list(all_libs))[:15])}`\n\n"

    # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Ñ–∞–π–ª–∞–º
    for file_path, deps in state['dependencies'].items():
        rel_path = os.path.relpath(file_path)
        content += f"### üîß {rel_path}\n\n"

        if deps['libraries']:
            libs_str = ', '.join([f"`{lib}`" for lib in deps['libraries'][:10]])
            if len(deps['libraries']) > 10:
                libs_str += f" ... –∏ –µ—â–µ {len(deps['libraries']) - 10}"
            content += f"**–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏:** {libs_str}\n\n"

        if deps['functions']:
            funcs_str = ', '.join([f"`{func}`" for func in deps['functions'][:15]])
            if len(deps['functions']) > 15:
                funcs_str += f" ... –∏ –µ—â–µ {len(deps['functions']) - 15}"
            content += f"**–§—É–Ω–∫—Ü–∏–∏:** {funcs_str}\n\n"

        content += "---\n\n"

    return content


def generate_prompts_section(state: ParserState) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤"""
    content = ""

    if not state['prompts']:
        return "–ü—Ä–æ–º–ø—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\n\n"

    total_prompts = sum(len(prompts) for prompts in state['prompts'].values())
    content += f"**–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –ò–ò:** {total_prompts}\n\n"

    for file_path, prompts in state['prompts'].items():
        if not prompts:
            continue

        rel_path = os.path.relpath(file_path)
        content += f"### üéØ {rel_path} ({len(prompts)} –ø—Ä–æ–º–ø—Ç–æ–≤)\n\n"

        for i, prompt in enumerate(prompts[:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
            # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
            if len(prompt) > 500:
                truncated = prompt[:500] + "..."
            else:
                truncated = prompt

            content += f"**–ü—Ä–æ–º–ø—Ç {i}:**\n"
            content += f"```\n{truncated}\n```\n\n"

        if len(prompts) > 3:
            content += f"*... –∏ –µ—â–µ {len(prompts) - 3} –ø—Ä–æ–º–ø—Ç–æ–≤*\n\n"

        content += "---\n\n"

    return content


def generate_empty_files_section(state: ParserState) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–∏ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    content = "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã:\n\n"

    for file_path in state['empty_files']:
        rel_path = os.path.relpath(file_path)
        content += f"- `{rel_path}`\n"

    content += f"\n**–í—Å–µ–≥–æ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤:** {len(state['empty_files'])}\n\n"
    return content


def generate_recommendations_section(state: ParserState) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    content = "## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n"

    recommendations = []

    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–º–ø—Ç–æ–≤
    total_prompts = sum(len(prompts) for prompts in state['prompts'].values())
    if total_prompts == 0:
        recommendations.append(
            "üéØ **–ü—Ä–æ–º–ø—Ç—ã:** –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –ò–ò. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —à–∞–±–ª–æ–Ω—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ò–ò.")
    elif total_prompts < 5:
        recommendations.append(
            "üéØ **–ü—Ä–æ–º–ø—Ç—ã:** –ù–∞–π–¥–µ–Ω–æ –º–∞–ª–æ –ø—Ä–æ–º–ø—Ç–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ò–ò-–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π.")
    else:
        recommendations.append(f"üéØ **–ü—Ä–æ–º–ø—Ç—ã:** –ù–∞–π–¥–µ–Ω–æ {total_prompts} –ø—Ä–æ–º–ø—Ç–æ–≤ - —Ö–æ—Ä–æ—à–∏–π —É—Ä–æ–≤–µ–Ω—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ò–ò.")

    # –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
    if not state['project_descriptions']:
        recommendations.append(
            "üìñ **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:** –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–∑–¥–∞—Ç—å README –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é.")

    if not state['business_requirements']:
        recommendations.append(
            "üíº **–ë–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:** –ù–µ –Ω–∞–π–¥–µ–Ω—ã –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫—É –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è.")

    # –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    if len(state['dependencies']) < len(state['files_list']) * 0.5:
        recommendations.append(
            "üîß **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** –ù–µ –≤—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏. –í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –∏–ª–∏ –¥–æ—Å—Ç—É–ø–æ–º –∫ —Ñ–∞–π–ª–∞–º.")

    # –ê–Ω–∞–ª–∏–∑ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤
    if state['empty_files']:
        recommendations.append(
            f"‚ö†Ô∏è **–ü—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã:** –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(state['empty_files'])} –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Ö —É–¥–∞–ª–∏—Ç—å –∏–ª–∏ –∑–∞–ø–æ–ª–Ω–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º.")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    if any('api_key' in str(deps).lower() or 'password' in str(deps).lower() for deps in
           state['dependencies'].values()):
        recommendations.append(
            "üîí **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:** –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–ª—é—á–µ–π API –∏–ª–∏ –ø–∞—Ä–æ–ª–µ–π –≤ –∫–æ–¥–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö.")

    for rec in recommendations:
        content += f"- {rec}\n"

    content += "\n"

    # –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
    content += "### üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏\n\n"
    content += "1. **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:** –£–ª—É—á—à–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –∏ –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π\n"
    content += "2. **–ü—Ä–æ–º–ø—Ç—ã:** –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–ø—Ç—ã –ò–ò\n"
    content += "3. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** –ü—Ä–æ–≤–µ—Å—Ç–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n"
    content += "4. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:** –î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç—ã –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤\n"
    content += "5. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:** –í–Ω–µ–¥—Ä–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–∞–±–æ—Ç—ã –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤\n\n"

    return content


def build_graph(llm):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –Ω–æ–≤—ã–º–∏ —É–∑–ª–∞–º–∏"""
    graph = StateGraph(ParserState)

    graph.add_node("get_files", get_files_node)
    graph.add_node("analyze_files", analyze_files_node)
    graph.add_node("llm_analysis", lambda state: llm_analysis_node(state, llm))
    graph.add_node("compile_report", compile_report_node)

    graph.set_entry_point("get_files")
    graph.add_edge("get_files", "analyze_files")
    graph.add_edge("analyze_files", "llm_analysis")
    graph.add_edge("llm_analysis", "compile_report")
    graph.add_edge("compile_report", END)

    return graph.compile()


def generate_summary_statistics():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –ª–æ–≥–æ–≤"""
    try:
        if os.path.exists("analysis_details.json"):
            with open("analysis_details.json", 'r', encoding='utf-8') as f:
                details = json.load(f)

            logger.info("=" * 50)
            logger.info("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê")
            logger.info("=" * 50)
            logger.info(f"üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {details.get('total_files', 0)}")
            logger.info(f"üéØ –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –ò–ò: {details.get('prompts_count', 0)}")
            logger.info(f"üîß –§–∞–π–ª–æ–≤ —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏: {details.get('dependencies_count', 0)}")
            logger.info(f"üìã –§–∞–π–ª–æ–≤ —Å –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏: {details.get('business_requirements_count', 0)}")
            logger.info(f"‚öôÔ∏è –§–∞–π–ª–æ–≤ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏: {details.get('requirements_count', 0)}")
            logger.info(f"üìñ –§–∞–π–ª–æ–≤ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –ø—Ä–æ–µ–∫—Ç–∞: {details.get('project_descriptions_count', 0)}")
            logger.info(f"‚ö†Ô∏è –ü—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤: {details.get('empty_files', 0)}")
            logger.info("=" * 50)

            if details.get('file_stats'):
                logger.info("üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ø–æ —Ç–∏–ø–∞–º:")
                for ext, count in sorted(details['file_stats'].items(), key=lambda x: x[1], reverse=True):
                    logger.info(f"  {ext}: {count} —Ñ–∞–π–ª–æ–≤")

            logger.info("=" * 50)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")


if __name__ == "__main__":
    try:
        config = LLMConfig.from_env()
        logger.info(f"üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LLM: {config.provider.value}, –º–æ–¥–µ–ª—å: {config.model}")

        llm = create_llm_client(config)
        logger.info("‚úÖ LLM –∫–ª–∏–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω")

        app = build_graph(llm)

        # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        directory = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞: ").strip()
        if not directory:
            directory = "."

        if not os.path.exists(directory):
            logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {directory}")
            exit(1)

        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {directory}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤
        supported = get_supported_extensions()
        logger.info("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤:")
        for category, extensions in supported.items():
            logger.info(f"  {category}: {', '.join(extensions)}")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
        result = app.invoke({"directory_path": directory})

        # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print("\n" + "=" * 60)
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê")
        print("=" * 60)
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(result['files_list'])}")
        print(f"–ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –ò–ò: {sum(len(prompts) for prompts in result['prompts'].values())}")
        print(f"–§–∞–π–ª–æ–≤ —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏: {len(result['dependencies'])}")
        print(f"–ë–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: {len(result['business_requirements'])}")
        print(f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: {len(result['requirements'])}")
        print(f"–û–ø–∏—Å–∞–Ω–∏–π –ø—Ä–æ–µ–∫—Ç–∞: {len(result['project_descriptions'])}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        report_filename = "parsed_project_enhanced.md"
        with open(report_filename, "w", encoding="utf-8") as f:
            f.write(result["final_report"])

        print(f"\n–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {report_filename}")
        print(f"LLM –∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: llm_analysis.txt")
        print(f"–î–µ—Ç–∞–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: analysis_details.json")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        generate_summary_statistics()

        logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")

    except KeyboardInterrupt:
        logger.info("‚ùå –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except ImportError as e:
        logger.error(f"‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {e}")
        print(f"\nüîß –î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:")
        print("pip install langchain-gigachat PyPDF2 python-docx javalang markdown pandas openpyxl")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

        # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–æ—Ç—è –±—ã —á–∞—Å—Ç–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        try:
            with open("error_report.txt", "w", encoding="utf-8") as f:
                f.write(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}\n")
                f.write(f"–¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}\n")
                f.write(f"–í—Ä–µ–º—è –æ—à–∏–±–∫–∏: {import_datetime.datetime.now()}\n")
            print("üîß –ß–∞—Å—Ç–∏—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ error_report.txt")
        except:
            pass