import json
from typing import TypedDict, List, Dict, Optional

from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import os
import logging
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import load_env_file, LLMProvider, LLMConfig
from prompts import PROMPTS

# –ò–º–ø–æ—Ä—Ç –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –ø–∞—Ä—Å–µ—Ä–æ–≤
from parsers import (
    extract_dependencies,
    extract_prompts,
    load_requirements,
    get_project_structure,
    extract_text_from_pdf,
    extract_text_from_docx,
    extract_text_from_txt,
    smart_parse_txt_bpmn,
    parse_md,
    extract_java_dependencies,
    extract_js_dependencies,
    extract_cpp_dependencies
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ .env —Ñ–∞–π–ª–∞
load_env_file()

def create_llm_client(config: LLMConfig):
    """–°–æ–∑–¥–∞–Ω–∏–µ LLM –∫–ª–∏–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    try:
        if config.provider == LLMProvider.GIGACHAT:
            from langchain_gigachat import GigaChat
            if not (config.cert_file and config.key_file):
                raise ValueError("–î–ª—è GigaChat –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã cert_file –∏ key_file")
            return GigaChat(
                base_url=config.base_url,
                cert_file=config.cert_file,
                key_file=config.key_file,
                model=config.model,
                temperature=config.temperature,
                top_p=config.top_p,
                verify_ssl_certs=config.verify_ssl_certs,
                profanity_check=config.profanity_check,
                streaming=config.streaming
            )
        else:
            from langchain_openai import OpenAI
            return OpenAI(
                base_url=config.base_url,
                model=config.model,
                temperature=config.temperature,
                max_tokens=config.max_tokens,
                timeout=config.timeout,
                api_key=config.api_key
            )
    except ImportError as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: {e}")
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞: {e}")
        raise

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≥—Ä–∞—Ñ–∞
class ParserState(TypedDict):
    directory_path: str
    files_list: List[str]
    empty_files: List[str]
    dependencies: Dict[str, Dict[str, List[str]]]
    prompts: Dict[str, List[str]]
    requirements: Dict[str, str]
    analysis: str
    final_report: str
    project_structure: Dict[str, List[str]]

def chunk_text(text: str, chunk_size: int = 80000, overlap: float = 0.2) -> List[str]:
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
    chunks = []
    overlap_size = int(chunk_size * overlap)
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start += chunk_size - overlap_size
    return chunks

def get_files_node(state: ParserState) -> ParserState:
    files = []
    empty_files = []
    code_extensions = {'.py', '.js', '.java', '.cpp'}
    text_extensions = {'.txt', '.md', '.yaml', '.yml', '.json', '.docx', '.pdf', '.bpmn'}

    project_structure = get_project_structure(state['directory_path'])

    for root, dirs, filenames in os.walk(state['directory_path']):
        if '.git' in dirs:
            dirs.remove('.git')
        for filename in filenames:
            file_path = os.path.join(root, filename)
            ext = os.path.splitext(filename)[1].lower()
            if ext not in code_extensions and ext not in text_extensions:
                continue
            try:
                if os.path.getsize(file_path) == 0:
                    empty_files.append(file_path)
                    continue
                files.append(file_path)
            except OSError:
                continue

    state['files_list'] = files
    state['empty_files'] = empty_files
    state['project_structure'] = project_structure
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(files)} –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ {len(empty_files)} –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤")
    return state

def analyze_files_node(state: ParserState) -> ParserState:
    dependencies = {}
    prompts = {}
    requirements = {}

    with ThreadPoolExecutor(max_workers=4) as executor:
        code_files = [f for f in state['files_list'] if os.path.splitext(f)[1].lower() in {'.py', '.js', '.java', '.cpp'}]
        text_files = [f for f in state['files_list'] if os.path.splitext(f)[1].lower() in {'.txt', '.md', '.yaml', '.yml', '.json', '.docx', '.pdf', '.bpmn'}]

        # –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        future_to_file = {executor.submit(extract_dependencies, file): file for file in code_files}
        for future in as_completed(future_to_file):
            file = future_to_file[future]
            try:
                dependencies[file] = future.result()
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π {file}: {e}")
                dependencies[file] = {"libraries": [], "functions": []}

        # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π...
        # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

    state['dependencies'] = dependencies
    state['prompts'] = prompts
    state['requirements'] = requirements
    return state

def llm_analysis_node(state: ParserState, llm) -> ParserState:
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM
        dep_content = "\n".join([f"–§–∞–π–ª {file}: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏: {deps['libraries']}, –§—É–Ω–∫—Ü–∏–∏: {deps['functions']}"
                                for file, deps in state['dependencies'].items()])
        prompt_content = "\n".join([f"–§–∞–π–ª {file}: {', '.join(prompts)}"
                                   for file, prompts in state['prompts'].items() if prompts])
        req_content = "\n".join([f"–§–∞–π–ª {file}: {req}"
                                for file, req in state['requirements'].items()])
        structure_content = "\n".join([f"–ü–∞–ø–∫–∞ {dir}: {', '.join(files)}"
                                     for dir, files in state['project_structure'].items()])

        all_content = f"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:\n{structure_content}\n\n–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:\n{dep_content}\n\n–ü—Ä–æ–º–ø—Ç—ã:\n{prompt_content}\n\n–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n{req_content}"

        # –ß–∞–Ω–∫–∏–Ω–≥ –≤–º–µ—Å—Ç–æ –æ–±—Ä–µ–∑–∞–Ω–∏—è
        max_input_size = 100000
        chunks = chunk_text(all_content, chunk_size=80000, overlap=0.2)

        system_prompt_template = ChatPromptTemplate.from_template(PROMPTS['system_prompt'])
        chain = system_prompt_template | llm | StrOutputParser()

        analysis_chunks = []
        for chunk in chunks:
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –ø–µ—Ä–µ–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –≤–º–µ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∏
            analysis_chunk = chain.invoke({"content": chunk})
            analysis_chunks.append(analysis_chunk)

        # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤
        summary_prompt = ChatPromptTemplate.from_template("–°—É–º–º–∏—Ä—É–π –∏ –æ–±—ä–µ–¥–∏–Ω–∏ —ç—Ç–∏ –∞–Ω–∞–ª–∏–∑—ã –≤ coherent–Ω—ã–π –æ—Ç—á–µ—Ç: {analyses}")
        summary_chain = summary_prompt | llm | StrOutputParser()
        analysis = summary_chain.invoke({"analyses": "\n\n".join(analysis_chunks)})

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å –ø–æ–º–æ—â—å—é judge
        judge_prompt_template = ChatPromptTemplate.from_template(PROMPTS['judge'])
        judge_chain = judge_prompt_template | llm | StrOutputParser()

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è judge
        judge_input = {
            "system_prompt": PROMPTS['system_prompt'],
            "analysis": analysis,
            "content": all_content
        }

        # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∞–º–∏
        if len(str(judge_input)) > max_input_size:
            judge_chunks = chunk_text(str(judge_input), chunk_size=80000, overlap=0.2)
            judge_outputs = []
            for judge_chunk in judge_chunks:
                # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ —Å–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
                judge_output = judge_chain.invoke({
                    "system_prompt": PROMPTS['system_prompt'],
                    "analysis": analysis[:10000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥–ª—è judge
                    "content": judge_chunk
                })
                judge_outputs.append(judge_output)
            judge_output = "\n".join(judge_outputs)
        else:
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –ø–µ—Ä–µ–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            judge_output = judge_chain.invoke(judge_input)

        try:
            judge_result = json.loads(judge_output)
        except json.JSONDecodeError:
            logger.error("–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ judge output")
            judge_result = {"status": "bad", "missing": []}

        final_analysis = analysis
        if judge_result['status'] == "bad":
            missing = judge_result.get('missing', [])
            if "prompts" in missing:
                final_analysis += "\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã (–∏–∑ –ø–∞—Ä—Å–µ—Ä–∞):\n" + prompt_content
            if "dependencies" in missing:
                final_analysis += "\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–∏–∑ –ø–∞—Ä—Å–µ—Ä–∞):\n" + dep_content
            if "structure" in missing:
                final_analysis += "\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ (–∏–∑ –ø–∞—Ä—Å–µ—Ä–∞):\n" + structure_content
            if "requirements" in missing:
                final_analysis += "\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (–∏–∑ –ø–∞—Ä—Å–µ—Ä–∞):\n" + req_content

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
        analysis_file = "llm_analysis.txt"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            f.write(final_analysis)
        state['analysis_file'] = analysis_file
        state['analysis'] = final_analysis
        logger.info("LLM –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ LLM –∞–Ω–∞–ª–∏–∑–µ: {e}")
        error_msg = f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"
        if "404" in str(e):
            error_msg += "\n–í–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞: –ù–µ–≤–µ—Ä–Ω—ã–π URL API –∏–ª–∏ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä LLM."
        with open("llm_analysis.txt", 'w', encoding='utf-8') as f:
            f.write(error_msg)
        state['analysis_file'] = "llm_analysis.txt"
        state['analysis'] = error_msg
    return state

def compile_report_node(state: ParserState) -> ParserState:
    report = "# –û—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –ø—Ä–æ–µ–∫—Ç–∞\n\n"

    report += "## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞\n"
    for dir, files in state['project_structure'].items():
        report += f"### {dir}\n"
        report += "\n".join([f"- {file}" for file in files]) + "\n\n"

    report += "## –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–∏\n"
    for file, deps in state['dependencies'].items():
        report += f"### {file}\n"
        report += f"- –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏: {', '.join(deps['libraries']) if deps['libraries'] else '–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç'}\n"
        report += f"- –§—É–Ω–∫—Ü–∏–∏: {', '.join(deps['functions']) if deps['functions'] else '–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç'}\n\n"

    report += "## –ü—Ä–æ–º–ø—Ç—ã\n"
    for file, prompts in state['prompts'].items():
        if prompts:
            report += f"### {file}\n"
            report += "\n".join([f"- {prompt}" for prompt in prompts]) + "\n\n"

    report += "## –ë–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è\n"
    for file, req in state['requirements'].items():
        max_req_display = 10000
        if len(req) > max_req_display:
            # –ß–∞–Ω–∫–∏–Ω–≥ –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –≤ –æ—Ç—á–µ—Ç–µ
            chunks = chunk_text(req, chunk_size=5000, overlap=0.2)
            req_summary = "\n".join(chunks[:1]) + "\n... [—Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–æ, –∏—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {} —Å–∏–º–≤–æ–ª–æ–≤]".format(len(req))
            report += f"### {file}\n{req_summary}\n\n"
        else:
            report += f"### {file}\n{req}\n\n"

    report += "## –ê–Ω–∞–ª–∏–∑ LLM\n"
    report += state['analysis'] + "\n"

    if state['empty_files']:
        report += "## –ü—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã\n" + "\n".join(state['empty_files']) + "\n"

    state['final_report'] = report
    return state

def build_graph(llm):
    graph = StateGraph(ParserState)

    graph.add_node("get_files", get_files_node)
    graph.add_node("analyze_files", analyze_files_node)
    graph.add_node("llm_analysis", lambda state: llm_analysis_node(state, llm))
    graph.add_node("compile_report", compile_report_node)

    graph.set_entry_point("get_files")
    graph.add_edge("get_files", "analyze_files")
    graph.add_edge("analyze_files", "llm_analysis")
    graph.add_edge("llm_analysis", "compile_report")
    graph.add_edge("compile_report", END)

    return graph.compile()

if __name__ == "__main__":
    try:
        config = LLMConfig.from_env()
        logger.info(f"üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LLM: {config.provider.value}, –º–æ–¥–µ–ª—å: {config.model}")

        llm = create_llm_client(config)
        logger.info("‚úÖ LLM –∫–ª–∏–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω")

        app = build_graph(llm)
        directory = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞: ")

        result = app.invoke({"directory_path": directory})
        print(result["final_report"])

        with open("parsed_project_–∞–≥–µ–Ω—Ç95.md", "w", encoding="utf-8") as f:
            f.write(result["final_report"])

        logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    except ImportError as e:
        logger.error(f"‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {e}")
        print(f"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install langchain-gigachat PyPDF2 python-docx javalang markdown")
    except Exception as e:
        logger.error(f"‚ùå –§–∞—Ç–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"–û—à–∏–±–∫–∞: {e}")