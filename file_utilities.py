"""
–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
"""

import os
import re
import json
import logging
from typing import Dict, List, Tuple, Optional, Any
import pandas as pd
from pathlib import Path

logger = logging.getLogger(__name__)

class FileAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.supported_formats = {
            'text': ['.txt', '.md', '.rst'],
            'document': ['.docx', '.pdf', '.rtf'],
            'data': ['.csv', '.xlsx', '.xls', '.json', '.yaml', '.yml'],
            'code': ['.py', '.js', '.ts', '.java', '.cpp', '.c', '.h'],
            'config': ['.ini', '.conf', '.config', '.env', '.toml'],
            'markup': ['.xml', '.html', '.htm', '.bpmn']
        }
    
    def analyze_file_content(self, file_path: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        try:
            file_info = {
                'path': file_path,
                'name': os.path.basename(file_path),
                'extension': os.path.splitext(file_path)[1].lower(),
                'size': os.path.getsize(file_path),
                'type': self._determine_file_type(file_path),
                'encoding': self._detect_encoding(file_path),
                'content_indicators': self._analyze_content_indicators(file_path),
                'potential_ai_content': self._detect_ai_content(file_path),
                'business_relevance': self._assess_business_relevance(file_path),
                'technical_complexity': self._assess_technical_complexity(file_path)
            }
            return file_info
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return {'path': file_path, 'error': str(e)}
    
    def _determine_file_type(self, file_path: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞"""
        ext = os.path.splitext(file_path)[1].lower()
        for file_type, extensions in self.supported_formats.items():
            if ext in extensions:
                return file_type
        return 'unknown'
    
    def _detect_encoding(self, file_path: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ —Ñ–∞–π–ª–∞"""
        try:
            import chardet
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)  # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 10KB
                result = chardet.detect(raw_data)
                return result.get('encoding', 'unknown')
        except ImportError:
            return 'utf-8'  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º UTF-8 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        except Exception:
            return 'unknown'
    
    def _analyze_content_indicators(self, file_path: str) -> Dict[str, bool]:
        """–ê–Ω–∞–ª–∏–∑ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        indicators = {
            'contains_prompts': False,
            'contains_business_logic': False,
            'contains_api_keys': False,
            'contains_configurations': False,
            'contains_data_structures': False,
            'contains_documentation': False
        }
        
        try:
            # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            if self._is_text_file(file_path):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(5000).lower()  # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 5KB
                    
                    # –ò—â–µ–º –ø—Ä–æ–º–ø—Ç—ã
                    prompt_keywords = ['prompt', '–ø—Ä–æ–º–ø—Ç', 'template', '—à–∞–±–ª–æ–Ω', 'system', 'user', 'assistant']
                    indicators['contains_prompts'] = any(keyword in content for keyword in prompt_keywords)
                    
                    # –ò—â–µ–º –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫—É
                    business_keywords = ['—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', 'business', 'process', 'workflow', '–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å']
                    indicators['contains_business_logic'] = any(keyword in content for keyword in business_keywords)
                    
                    # –ò—â–µ–º API –∫–ª—é—á–∏
                    api_patterns = ['api_key', 'secret', 'token', 'password', 'credential']
                    indicators['contains_api_keys'] = any(pattern in content for pattern in api_patterns)
                    
                    # –ò—â–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                    config_keywords = ['config', 'setting', 'parameter', 'environment']
                    indicators['contains_configurations'] = any(keyword in content for keyword in config_keywords)
                    
                    # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
                    doc_keywords = ['readme', 'documentation', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'manual', 'guide']
                    indicators['contains_documentation'] = any(keyword in content for keyword in doc_keywords)
                    
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ {file_path}: {e}")
        
        return indicators
    
    def _detect_ai_content(self, file_path: str) -> Dict[str, Any]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ò–ò-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        ai_indicators = {
            'has_llm_interactions': False,
            'has_prompt_templates': False,
            'has_ai_workflows': False,
            'confidence_level': 0.0,
            'detected_ai_frameworks': []
        }
        
        try:
            if self._is_text_file(file_path):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(10000).lower()
                    
                    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ò–ò-–∫–æ–Ω—Ç–µ–Ω—Ç–∞
                    llm_patterns = [
                        r'openai', r'chatgpt', r'gpt-[0-9]', r'claude', r'anthropic',
                        r'langchain', r'llama', r'gemini', r'palm', r'–±–∞—Ä–¥'
                    ]
                    
                    prompt_patterns = [
                        r'chatprompttemplate', r'prompttemplate', r'system.*prompt',
                        r'user.*prompt', r'assistant.*prompt', r'–ø—Ä–æ–º–ø—Ç.*—à–∞–±–ª–æ–Ω'
                    ]
                    
                    workflow_patterns = [
                        r'chain\.invoke', r'agent\.run', r'crew\.kickoff',
                        r'workflow', r'pipeline', r'orchestrat'
                    ]
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    llm_matches = sum(1 for pattern in llm_patterns if re.search(pattern, content))
                    prompt_matches = sum(1 for pattern in prompt_patterns if re.search(pattern, content))
                    workflow_matches = sum(1 for pattern in workflow_patterns if re.search(pattern, content))
                    
                    ai_indicators['has_llm_interactions'] = llm_matches > 0
                    ai_indicators['has_prompt_templates'] = prompt_matches > 0
                    ai_indicators['has_ai_workflows'] = workflow_matches > 0
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                    total_matches = llm_matches + prompt_matches + workflow_matches
                    ai_indicators['confidence_level'] = min(total_matches / 10.0, 1.0)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏
                    frameworks = []
                    if 'langchain' in content:
                        frameworks.append('LangChain')
                    if 'crewai' in content or 'crew' in content:
                        frameworks.append('CrewAI')
                    if 'openai' in content:
                        frameworks.append('OpenAI')
                    if 'anthropic' in content or 'claude' in content:
                        frameworks.append('Anthropic')
                    
                    ai_indicators['detected_ai_frameworks'] = frameworks
                    
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ò–ò-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ {file_path}: {e}")
        
        return ai_indicators
    
    def _assess_business_relevance(self, file_path: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ –±–∏–∑–Ω–µ—Å-—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞"""
        filename = os.path.basename(file_path).lower()
        
        # –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        high_relevance_indicators = [
            'requirements', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', 'business', '–±–∏–∑–Ω–µ—Å',
            'process', '–ø—Ä–æ—Ü–µ—Å—Å', 'workflow', 'specification',
            '—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è', 'use_case', '–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ_–∏—Å—Ç–æ—Ä–∏–∏'
        ]
        
        # –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        medium_relevance_indicators = [
            'config', 'settings', 'readme', 'documentation',
            '–æ–ø–∏—Å–∞–Ω–∏–µ', 'manual', 'guide', 'api'
        ]
        
        # –ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        low_relevance_indicators = [
            'test', '—Ç–µ—Å—Ç', 'debug', 'temp', 'tmp', 'cache',
            'log', 'backup', '__pycache__'
        ]
        
        for indicator in high_relevance_indicators:
            if indicator in filename:
                return 0.9
        
        for indicator in medium_relevance_indicators:
            if indicator in filename:
                return 0.6
        
        for indicator in low_relevance_indicators:
            if indicator in filename:
                return 0.2
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
        try:
            if self._is_text_file(file_path):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(2000).lower()
                    
                    business_keywords_count = sum(1 for keyword in [
                        '–±–∏–∑–Ω–µ—Å', 'business', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', 'requirements',
                        '–ø—Ä–æ—Ü–µ—Å—Å', 'process', '–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å', 'user',
                        '–∫–ª–∏–µ–Ω—Ç', 'client', '–∑–∞–∫–∞–∑—á–∏–∫', 'customer'
                    ] if keyword in content)
                    
                    if business_keywords_count >= 3:
                        return 0.8
                    elif business_keywords_count >= 1:
                        return 0.5
        except Exception:
            pass
        
        return 0.3  # –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _assess_technical_complexity(self, file_path: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞"""
        ext = os.path.splitext(file_path)[1].lower()
        
        # –í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        if ext in ['.py', '.java', '.cpp', '.c', '.js', '.ts']:
            complexity = 0.8
        # –°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        elif ext in ['.json', '.yaml', '.yml', '.xml', '.sql']:
            complexity = 0.6
        # –ù–∏–∑–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        elif ext in ['.txt', '.md', '.csv']:
            complexity = 0.3
        else:
            complexity = 0.5
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∫–æ–¥–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        try:
            if ext in ['.py', '.js', '.java'] and self._is_text_file(file_path):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()[:100]  # –ü–µ—Ä–≤—ã–µ 100 —Å—Ç—Ä–æ–∫
                    
                    complexity_indicators = 0
                    for line in lines:
                        line_lower = line.lower().strip()
                        # –ò—â–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
                        if any(keyword in line_lower for keyword in [
                            'class ', 'async ', 'await ', 'lambda',
                            'decorator', 'inheritance', 'polymorphism',
                            'threading', 'multiprocessing', 'asyncio'
                        ]):
                            complexity_indicators += 1
                    
                    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
                    complexity = min(complexity + (complexity_indicators * 0.05), 1.0)
                    
        except Exception:
            pass
        
        return complexity
    
    def _is_text_file(self, file_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª —Ç–µ–∫—Å—Ç–æ–≤—ã–º"""
        text_extensions = [
            '.txt', '.md', '.py', '.js', '.ts', '.java', '.cpp', '.c', '.h',
            '.json', '.yaml', '.yml', '.xml', '.html', '.css', '.sql',
            '.ini', '.conf', '.config', '.env', '.toml', '.rst'
        ]
        ext = os.path.splitext(file_path)[1].lower()
        return ext in text_extensions

class ContentExtractor:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤"""
    
    @staticmethod
    def extract_ai_prompts_advanced(file_path: str) -> List[Dict[str, Any]]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        prompts = []
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # –†–∞–∑–ª–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
            patterns = [
                # ChatPromptTemplate –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                {
                    'pattern': r'ChatPromptTemplate\.from_template\s*\(\s*["\']([^"\']+)["\']',
                    'type': 'ChatPromptTemplate',
                    'framework': 'LangChain'
                },
                # PromptTemplate –ø–∞—Ç—Ç–µ—Ä–Ω—ã  
                {
                    'pattern': r'PromptTemplate\s*\([^)]*?template\s*=\s*["\']([^"\']+)["\']',
                    'type': 'PromptTemplate', 
                    'framework': 'LangChain'
                },
                # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
                {
                    'pattern': r'system.*prompt.*["\']([^"\']{50,})["\']',
                    'type': 'SystemPrompt',
                    'framework': 'Generic'
                },
                # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã
                {
                    'pattern': r'user.*prompt.*["\']([^"\']{30,})["\']',
                    'type': 'UserPrompt',
                    'framework': 'Generic'
                },
                # –ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
                {
                    'pattern': r'["\'{3}](.*?(?:analyze|–∞–Ω–∞–ª–∏–∑|–∑–∞–¥–∞—á–∞|instruction).*?)["\'{3}]',
                    'type': 'MultilinePrompt',
                    'framework': 'Generic'
                }
            ]
            
            for pattern_info in patterns:
                matches = re.findall(pattern_info['pattern'], content, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    prompt_data = {
                        'text': match.strip(),
                        'type': pattern_info['type'],
                        'framework': pattern_info['framework'],
                        'file_path': file_path,
                        'length': len(match.strip()),
                        'language': 'ru' if any(ru_word in match.lower() for ru_word in ['—Ç—ã', '–∞–Ω–∞–ª–∏–∑', '–∑–∞–¥–∞—á–∞', '—Å–æ–∑–¥–∞–π']) else 'en',
                        'complexity': ContentExtractor._assess_prompt_complexity(match)
                    }
                    prompts.append(prompt_data)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ {file_path}: {e}")
        
        return prompts
    
    @staticmethod
    def _assess_prompt_complexity(prompt_text: str) -> str:
        """–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–æ–º–ø—Ç–∞"""
        length = len(prompt_text)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        complexity_indicators = 0
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        if '{' in prompt_text and '}' in prompt_text:
            complexity_indicators += 1  # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        
        if any(word in prompt_text.lower() for word in ['step by step', '–ø–æ—à–∞–≥–æ–≤–æ', 'analyze', '–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π']):
            complexity_indicators += 1  # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        
        if any(word in prompt_text.lower() for word in ['format:', '—Ñ–æ—Ä–º–∞—Ç:', 'example:', '–ø—Ä–∏–º–µ—Ä:']):
            complexity_indicators += 1  # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        
        if prompt_text.count('\n') > 5:
            complexity_indicators += 1  # –ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ—Å—Ç—å
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        if length < 100:
            return 'simple'
        elif length < 500 and complexity_indicators <= 1:
            return 'medium'
        elif length < 1000 and complexity_indicators <= 2:
            return 'complex'
        else:
            return 'advanced'
    
    @staticmethod
    def extract_business_entities(file_path: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å-—Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        entities = {
            'roles': [],
            'processes': [],
            'requirements': [],
            'constraints': [],
            'stakeholders': [],
            'systems': [],
            'goals': []
        }
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            ext = os.path.splitext(file_path)[1].lower()
            
            if ext == '.csv':
                entities.update(ContentExtractor._extract_from_csv(file_path))
            elif ext in ['.xlsx', '.xls']:
                entities.update(ContentExtractor._extract_from_excel(file_path))
            elif ext in ['.txt', '.md']:
                entities.update(ContentExtractor._extract_from_text(file_path))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –±–∏–∑–Ω–µ—Å-—Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ {file_path}: {e}")
        
        return entities
    
    @staticmethod
    def _extract_from_csv(file_path: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ CSV"""
        entities = {'roles': [], 'processes': [], 'requirements': []}
        
        try:
            df = pd.read_csv(file_path, encoding='utf-8')
            
            # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å —Ä–æ–ª—è–º–∏
            role_columns = [col for col in df.columns if any(keyword in col.lower() 
                          for keyword in ['—Ä–æ–ª—å', 'role', '–¥–æ–ª–∂–Ω–æ—Å—Ç—å', 'position'])]
            
            for col in role_columns:
                entities['roles'].extend(df[col].dropna().astype(str).tolist())
            
            # –ò—â–µ–º –ø—Ä–æ—Ü–µ—Å—Å—ã
            process_columns = [col for col in df.columns if any(keyword in col.lower()
                             for keyword in ['–ø—Ä–æ—Ü–µ—Å—Å', 'process', 'workflow', '–∑–∞–¥–∞—á–∞', 'task'])]
            
            for col in process_columns:
                entities['processes'].extend(df[col].dropna().astype(str).tolist())
            
            # –ò—â–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
            req_columns = [col for col in df.columns if any(keyword in col.lower()
                          for keyword in ['—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ', 'requirement', 'spec', '—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è'])]
            
            for col in req_columns:
                entities['requirements'].extend(df[col].dropna().astype(str).tolist())
                
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ CSV {file_path}: {e}")
        
        return entities
    
    @staticmethod
    def _extract_from_excel(file_path: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ Excel"""
        entities = {'roles': [], 'processes': [], 'requirements': []}
        
        try:
            xl_file = pd.ExcelFile(file_path)
            
            for sheet_name in xl_file.sheet_names:
                df = pd.read_excel(file_path, sheet_name=sheet_name)
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É, —á—Ç–æ –∏ –¥–ª—è CSV
                sheet_entities = ContentExtractor._extract_from_csv_dataframe(df)
                
                for key in entities:
                    entities[key].extend(sheet_entities.get(key, []))
                    
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ Excel {file_path}: {e}")
        
        return entities
    
    @staticmethod
    def _extract_from_csv_dataframe(df: pd.DataFrame) -> Dict[str, List[str]]:
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ DataFrame"""
        entities = {'roles': [], 'processes': [], 'requirements': []}
        
        # –†–æ–ª–∏
        role_columns = [col for col in df.columns if any(keyword in str(col).lower() 
                      for keyword in ['—Ä–æ–ª—å', 'role', '–¥–æ–ª–∂–Ω–æ—Å—Ç—å', 'position'])]
        
        for col in role_columns:
            entities['roles'].extend(df[col].dropna().astype(str).tolist())
        
        # –ü—Ä–æ—Ü–µ—Å—Å—ã
        process_columns = [col for col in df.columns if any(keyword in str(col).lower()
                         for keyword in ['–ø—Ä–æ—Ü–µ—Å—Å', 'process', 'workflow', '–∑–∞–¥–∞—á–∞', 'task'])]
        
        for col in process_columns:
            entities['processes'].extend(df[col].dropna().astype(str).tolist())
        
        # –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è
        req_columns = [col for col in df.columns if any(keyword in str(col).lower()
                      for keyword in ['—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ', 'requirement', 'spec', '—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è'])]
        
        for col in req_columns:
            entities['requirements'].extend(df[col].dropna().astype(str).tolist())
        
        return entities
    
    @staticmethod
    def _extract_from_text(file_path: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        entities = {'roles': [], 'processes': [], 'requirements': [], 'goals': [], 'constraints': []}
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            lines = content.split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                line_lower = line.lower()
                
                # –ò—â–µ–º —Ä–æ–ª–∏
                if any(keyword in line_lower for keyword in ['—Ä–æ–ª—å:', 'role:', '–¥–æ–ª–∂–Ω–æ—Å—Ç—å:', '–∞–≥–µ–Ω—Ç', '–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å']):
                    entities['roles'].append(line)
                
                # –ò—â–µ–º –ø—Ä–æ—Ü–µ—Å—Å—ã
                elif any(keyword in line_lower for keyword in ['–ø—Ä–æ—Ü–µ—Å—Å:', 'process:', 'workflow:', '–∞–ª–≥–æ—Ä–∏—Ç–º']):
                    entities['processes'].append(line)
                
                # –ò—â–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
                elif any(keyword in line_lower for keyword in ['—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ:', 'requirement:', '–¥–æ–ª–∂–µ–Ω', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ']):
                    entities['requirements'].append(line)
                
                # –ò—â–µ–º —Ü–µ–ª–∏
                elif any(keyword in line_lower for keyword in ['—Ü–µ–ª—å:', 'goal:', '–∑–∞–¥–∞—á–∞:', 'objective:']):
                    entities['goals'].append(line)
                
                # –ò—â–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                elif any(keyword in line_lower for keyword in ['–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ:', 'constraint:', '–Ω–µ–ª—å–∑—è', '–∑–∞–ø—Ä–µ—â–µ–Ω–æ']):
                    entities['constraints'].append(line)
                    
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ {file_path}: {e}")
        
        return entities

class ReportGenerator:
    """–ö–ª–∞—Å—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–æ–≤"""
    
    @staticmethod
    def generate_file_analysis_report(file_analyses: List[Dict]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ñ–∞–π–ª–æ–≤"""
        report = "# üìä –û—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞\n\n"
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_files = len(file_analyses)
        ai_files = sum(1 for fa in file_analyses 
                      if fa.get('potential_ai_content', {}).get('confidence_level', 0) > 0.3)
        business_files = sum(1 for fa in file_analyses 
                           if fa.get('business_relevance', 0) > 0.7)
        
        report += f"## üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n"
        report += f"- **–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ:** {total_files}\n"
        report += f"- **–§–∞–π–ª–æ–≤ —Å –ò–ò-–∫–æ–Ω—Ç–µ–Ω—Ç–æ–º:** {ai_files}\n"  
        report += f"- **–§–∞–π–ª–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π –±–∏–∑–Ω–µ—Å-—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é:** {business_files}\n\n"
        
        # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤
        file_types = {}
        for fa in file_analyses:
            file_type = fa.get('type', 'unknown')
            file_types[file_type] = file_types.get(file_type, 0) + 1
        
        report += f"## üìÅ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤\n\n"
        for file_type, count in sorted(file_types.items(), key=lambda x: x[1], reverse=True):
            report += f"- **{file_type}:** {count} —Ñ–∞–π–ª–æ–≤\n"
        
        report += "\n"
        
        # –ò–ò-–∫–æ–Ω—Ç–µ–Ω—Ç –∞–Ω–∞–ª–∏–∑
        if ai_files > 0:
            report += f"## ü§ñ –ê–Ω–∞–ª–∏–∑ –ò–ò-–∫–æ–Ω—Ç–µ–Ω—Ç–∞\n\n"
            
            ai_frameworks = {}
            for fa in file_analyses:
                frameworks = fa.get('potential_ai_content', {}).get('detected_ai_frameworks', [])
                for framework in frameworks:
                    ai_frameworks[framework] = ai_frameworks.get(framework, 0) + 1
            
            if ai_frameworks:
                report += f"**–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ò–ò-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏:**\n"
                for framework, count in sorted(ai_frameworks.items(), key=lambda x: x[1], reverse=True):
                    report += f"- {framework}: {count} —Ñ–∞–π–ª–æ–≤\n"
            
            report += "\n"
        
        # –¢–æ–ø —Ñ–∞–π–ª—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        report += f"## üèÜ –¢–æ–ø —Ñ–∞–π–ª—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n\n"
        
        # –¢–æ–ø –ø–æ –ò–ò-–∫–æ–Ω—Ç–µ–Ω—Ç—É
        ai_sorted = sorted([fa for fa in file_analyses 
                           if fa.get('potential_ai_content', {}).get('confidence_level', 0) > 0],
                          key=lambda x: x.get('potential_ai_content', {}).get('confidence_level', 0),
                          reverse=True)[:5]
        
        if ai_sorted:
            report += f"### üéØ –¢–æ–ø —Ñ–∞–π–ª—ã —Å –ò–ò-–∫–æ–Ω—Ç–µ–Ω—Ç–æ–º\n\n"
            for i, fa in enumerate(ai_sorted, 1):
                confidence = fa.get('potential_ai_content', {}).get('confidence_level', 0)
                report += f"{i}. **{fa['name']}** (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})\n"
            report += "\n"
        
        # –¢–æ–ø –ø–æ –±–∏–∑–Ω–µ—Å-—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        business_sorted = sorted(file_analyses,
                               key=lambda x: x.get('business_relevance', 0),
                               reverse=True)[:5]
        
        report += f"### üíº –¢–æ–ø —Ñ–∞–π–ª—ã –ø–æ –±–∏–∑–Ω–µ—Å-—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n\n"
        for i, fa in enumerate(business_sorted, 1):
            relevance = fa.get('business_relevance', 0)
            report += f"{i}. **{fa['name']}** (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance:.2f})\n"
        
        return report
    
    @staticmethod
    def generate_prompt_analysis_report(prompts_data: List[Dict]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ –∞–Ω–∞–ª–∏–∑—É –ø—Ä–æ–º–ø—Ç–æ–≤"""
        if not prompts_data:
            return "# üéØ –ü—Ä–æ–º–ø—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\n\n–í –ø—Ä–æ–µ–∫—Ç–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –ò–ò.\n"
        
        report = "# üéØ –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–º–ø—Ç–æ–≤ –ò–ò\n\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
        total_prompts = len(prompts_data)
        avg_length = sum(p['length'] for p in prompts_data) / total_prompts
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        types_stats = {}
        complexity_stats = {}
        framework_stats = {}
        language_stats = {}
        
        for prompt in prompts_data:
            # –ü–æ —Ç–∏–ø–∞–º
            ptype = prompt.get('type', 'Unknown')
            types_stats[ptype] = types_stats.get(ptype, 0) + 1
            
            # –ü–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            complexity = prompt.get('complexity', 'unknown')
            complexity_stats[complexity] = complexity_stats.get(complexity, 0) + 1
            
            # –ü–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º
            framework = prompt.get('framework', 'Unknown')
            framework_stats[framework] = framework_stats.get(framework, 0) + 1
            
            # –ü–æ —è–∑—ã–∫–∞–º
            language = prompt.get('language', 'unknown')
            language_stats[language] = language_stats.get(language, 0) + 1
        
        report += f"## üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤\n\n"
        report += f"- **–í—Å–µ–≥–æ –ø—Ä–æ–º–ø—Ç–æ–≤:** {total_prompts}\n"
        report += f"- **–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞:** {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤\n"
        report += f"- **–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏:** {len(set(p['file_path'] for p in prompts_data))}\n\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        report += f"### üè∑Ô∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º\n\n"
        for ptype, count in sorted(types_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_prompts) * 100
            report += f"- **{ptype}:** {count} ({percentage:.1f}%)\n"
        report += "\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        report += f"### üß© –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\n\n"
        for complexity, count in sorted(complexity_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_prompts) * 100
            report += f"- **{complexity}:** {count} ({percentage:.1f}%)\n"
        report += "\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º
        if len(framework_stats) > 1:
            report += f"### üõ†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏\n\n"
            for framework, count in sorted(framework_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_prompts) * 100
                report += f"- **{framework}:** {count} ({percentage:.1f}%)\n"
            report += "\n"
        
        # –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–º–ø—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        report += f"## üìù –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–º–ø—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n\n"
        
        for ptype in types_stats:
            type_prompts = [p for p in prompts_data if p.get('type') == ptype]
            if type_prompts:
                report += f"### {ptype}\n\n"
                
                # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –∫–∞–∫ –ø—Ä–∏–º–µ—Ä
                example_prompt = max(type_prompts, key=lambda x: x['length'])
                
                prompt_text = example_prompt['text']
                if len(prompt_text) > 500:
                    prompt_text = prompt_text[:500] + "..."
                
                report += f"**–§–∞–π–ª:** `{os.path.basename(example_prompt['file_path'])}`\n"
                report += f"**–î–ª–∏–Ω–∞:** {example_prompt['length']} —Å–∏–º–≤–æ–ª–æ–≤\n"
                report += f"**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** {example_prompt.get('complexity', 'unknown')}\n\n"
                report += f"```\n{prompt_text}\n```\n\n"
        
        return report